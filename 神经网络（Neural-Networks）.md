
# 0. 什么是神经网络 (Neural Networks)
神经网络是实现AI 的一种技术手段，一种广泛用于机器学习（Machine Learning）和深度学习（Deep Learning）领域的计算模型/算法架构。

它受到人类大脑神经元（Neurons）和它们的互动方式的启发，它由多个层（Layers）组成，每层包含多个神经元，这些神经元通过权重（Weights）连接传递信息。

神经网络的训练过程基于机器学习的基本前提，即能够从数据中学习。通过向网络提供大量的数据样本（包括输入和期望的输出），神经网络可以学习到如何映射输入到输出，这种能力是通过调整内部结构（即权重）来实现的。

这一学习过程使用了机器学习中的核心概念，如损失函数（Loss Functions）、梯度下降（Gradient Descent）和反向传播算法（Backpropagation Algorithms）。这些都是机器学习领域的基本工具，用于训练模型以改进其性能。

# 1. 神经网络的层次结构
## 1.1  节点（Nodes）或神经元（Neurons）
节点是神经网络的基本单位，也称为神经元。每个节点接收输入信号，进行加权求和和非线性变换，然后将结果传递给下一层的节点。节点的主要功能包括：

1. **接收输入**：从前一层的所有节点接收输入信号。
2. **加权求和**：对接收到的输入信号乘以相应的权重并求和。
3. **应用激活函数**：对加权求和的结果应用激活函数，生成节点的输出。
4. **传递输出**：将输出信号传递给下一层的节点。

## 1.2  层（Layers）

层是由多个节点组成的一个集合，在神经网络中起着分层处理输入数据的作用。每一层中的节点执行相同类型的计算，但每个节点有各自的权重和偏置。

一个典型的神经网络由以下几个部分组成：

- **输入层（Input Layer）**：接收外部输入数据，每个节点代表一个输入特征。
- **隐藏层（Hidden Layers）**：位于输入层和输出层之间，用于特征提取和数据变换。一个神经网络可以有一个或多个隐藏层。
- **输出层（Output Layer）**：生成最终的预测结果，每个节点代表一个输出。


## 1.3  节点和层的关系

节点和层之间的关系可以概括如下：

1. **层是节点的集合**：每一层由多个节点组成，这些节点在同一层内进行相同类型的计算。
2. **数据传递和处理**：数据在神经网络中逐层传递，每一层的节点接收前一层的输出，进行计算后将结果传递给下一层的节点。
3. **层次结构**：神经网络的层次结构决定了数据处理的顺序和方式。输入数据经过输入层后，依次通过一个或多个隐藏层进行复杂的特征提取，最终通过输出层生成预测结果。


## 1.4 层与层之间的连接

层与层之间的连接由权重矩阵决定。每一层的节点与下一层的每个节点相连，连接的强度由权重值决定。在前向传播过程中，输入层的节点将输入数据传递给第一个隐藏层的节点，后者再将处理后的数据传递给下一个隐藏层，依此类推，直到输出层。


# 2. 权重
权重是连接神经网络中不同层节点的参数，用于调节输入信号在网络中的传递和变换强度。每个连接都有一个权重，表示输入信号对输出信号的影响力。
- **调节信号强度**：权重乘以输入信号后，再通过求和和激活函数的变换，决定了输出信号的值。不同的权重值代表输入信号的重要程度。
- **特征学习**：通过学习和调整权重，神经网络能够识别和提取输入数据中的重要特征。
- **优化**：在训练过程中，权重被调整以最小化预测误差。通过反向传播算法，权重的值不断更新，使得模型的预测更加准确。

## 2.1 权重的存在形式

权重并不存在于单独的节点内，而是存在于节点之间的连接上。具体来说，每一层的节点与下一层的节点之间的连接由一组权重参数表示。权重矩阵表示了两个层之间所有节点连接的权重值。


## 2.2 权重的初始化和调整


权重的初始化方法有多种，包括零初始化、随机初始化、Xavier初始化和He初始化等。这些方法旨在为模型提供一个良好的起点，以便更好地进行训练。


## 2.3 权重的训练调整

在训练过程中，权重通过优化算法进行调整。最常用的优化算法是梯度下降（Gradient Descent）及其变种（如随机梯度下降、Adam优化算法等）。训练步骤如下：

1. **前向传播**：计算网络的预测输出。
2. **计算损失**：通过损失函数衡量预测输出与真实标签之间的差距。
3. **反向传播**：计算损失函数对每个权重的梯度。
4. **更新权重**：使用梯度下降法更新权重，以最小化损失函数。

## 2.4 优化算法

为了提高训练效率和稳定性，可以使用各种优化算法，包括：

- **动量（Momentum）**：结合当前梯度和之前更新的方向，加速收敛。
- **AdaGrad**：根据过去的梯度调整学习率，适应性地进行参数更新。
- **RMSProp**：改进AdaGrad，使用指数加权平均计算梯度平方和，避免学习率过快减小。
- **Adam**：结合动量和RMSProp的优点，适应性地调整学习率和动量，广泛应用于各种神经网络训练。

## 2.5 权重在不同类型神经网络中的角色

不同类型的神经网络中，权重的具体作用可能有所不同：

- **前馈神经网络（Feedforward Neural Networks, FNNs）**：权重连接每一层的所有节点，负责将输入数据逐层传递和变换。
- **卷积神经网络（Convolutional Neural Networks, CNNs）**：权重是卷积核的参数，负责在局部感知野上提取特征。
- **递归神经网络（Recurrent Neural Networks, RNNs）**：权重用于处理序列数据，包括当前时间步的输入与前一时间步的隐藏状态之间的关系。

# 3. 偏置（Biases）

偏置是每个节点附加的一个参数，用于调整节点的输出独立于输入信号。偏置帮助神经网络学到更灵活的决策边界。

**作用**
- **调节输出**：偏置提供一个额外的自由度，使得神经网络能够更好地拟合数据。
- **避免零输出**：在输入信号为零的情况下，偏置确保节点仍然能够产生非零输出。

# 4. 激活函数（Activation Function）

激活函数（Activation Function）是神经网络中的一个关键组件，它引入了非线性变换，使得神经网络能够学习和表示复杂的非线性关系。如果没有激活函数，神经网络的每一层只进行线性变换，那么无论多少层的堆叠，整体仍然是一个线性变换。这将极大限制神经网络的表示能力。因此，激活函数对于神经网络的性能和能力至关重要。

线性（Linear）和非线性（Nonlinear）是数学和信号处理中两个基本的概念，这些概念在神经网络和机器学习中也具有重要意义。理解这两个概念有助于掌握为什么激活函数对神经网络的性能如此重要。
## 4.1 线性（Linear）vs  非线性（Nonlinear）
### 4.1.1 线性（Linear）

线性关系是指两个变量之间的关系可以用一个线性方程表示。对于一个变量 $x$ 和其对应的输出 $y$，线性关系可以表示为：
$y = mx + b$
其中，$m$ 是斜率，$b$ 是截距。这意味着如果我们绘制 $y$ 对 $x$ 的图像，它将是一条直线。

**特点**
- **叠加性**：线性系统满足叠加原理，即输入的线性组合会产生输出的线性组合。如果 $f(x_1) = y_1$​ 和$f(x_2) = y_2$，那么对于任何常数 $a$和 $b$，有 $f(ax_1 + bx_2) = ay_1 + by_2$​。
- **同质性**：线性系统满足同质性，即输入的放大会导致输出的相应放大。如果 $f(x) = y$，那么对于任何常数 $k$，有 $f(kx) = ky$。

**在神经网络中的应用**
在神经网络中，线性变换通常通过矩阵乘法和加法实现，例如输入与权重矩阵的乘积加上偏置：
$z = W \cdot x + b$

### 4.1.2 非线性（Nonlinear）
非线性关系是指两个变量之间的关系不能用一个简单的线性方程表示。非线性关系的数学表示形式可以非常多样，常见的形式包括多项式、指数函数、对数函数、三角函数等。
例如，对于变量 $x$ 和 $y$，非线性关系可以表示为：
$y=ax^2+bx+c$
其中，$a$、$b$、$c$ 是常数。这意味着如果我们绘制 $y$ 对 $x$ 的图像，它将不是一条直线，而是一个曲线。
**特点**
- **非叠加性**：非线性系统不满足叠加原理。如果 $f(x_1) = y_1​$ 和 $f(x_2) = y_2​$，那么 $f(ax_1 + bx_2) \neq ay_1 + by_2$​。
- **复杂性**：非线性系统可以表示复杂的关系和模式，能够捕捉到数据中的复杂结构和动态。

**为什么需要非线性**
如果神经网络只使用线性激活函数（例如，恒等函数 f(z)=z），那么无论网络有多少层，其最终输出仍然是输入的线性变换。也就是说，整个网络等效于一个单层的线性模型，无法捕捉数据中的复杂关系。因此，引入非线性激活函数使得网络具有更强的表达能力，能够学习和表示复杂的非线性关系，从而解决更复杂的问题。
## 4.2 常见的非线性激活函数
#### 4.2.1 Sigmoid 函数
$σ(z)=\frac{1}{1+e^{−z}} ​$

**特点**：
- 输出值在0到1之间，适用于概率预测。
- 在极值区间梯度较小，可能导致梯度消失问题。
- 计算复杂度较高。

**应用**：
- 常用于二分类问题的输出层。

#### 4.2.2 Tanh（双曲正切）函数
$tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$
​
**特点**：
- 输出值在-1到1之间。
- 相对于Sigmoid函数，Tanh函数的输出均值为0，使得数据更中心化。
- 也存在梯度消失问题，但在0附近的梯度较大，梯度消失问题稍好于Sigmoid。

**应用**：
- 常用于隐藏层。
### 4.2.3 ReLU（Rectified Linear Unit）函数
$ReLU(z)=max(0,z)$

**特点**：
- 计算简单，收敛速度快。
- 在正区间保持线性关系，在负区间输出为0。
- 解决了Sigmoid和Tanh的梯度消失问题。
- 可能导致部分神经元“死亡”，即在训练过程中某些神经元的输出始终为0，不再更新。

**应用**：
- 广泛用于隐藏层。
#### 4.2.4 Leaky ReLU 函数
$Leaky  ReLU(z)=max(αz,z)$

**特点**：
- 解决了ReLU的“死亡神经元”问题。
- 在负区间给定一个很小的斜率（通常为0.01）。

**应用**：
- 替代ReLU，在隐藏层中使用
#### 4.2.5 Softmax 函数
$Softmax(z_i​)=\frac{e^z_i}{∑_j​​e^z_j} ​$

**特点**：
- 将输出值转换为概率分布，总和为1。
- 适用于多分类问题。

**应用**：
- 常用于多分类问题的输出层。
#### 4.2.6 Swish 函数
$Swish(z)=z⋅σ(z)=\frac{z}{1+e^{−z}} ​​$

**特点**：
- 平滑的非线性函数，性能优于ReLU和Sigmoid。
- 由Google提出，结合了ReLU和Sigmoid的特点。

**应用**：
- 新型激活函数，在一些深度学习模型中表现出色。

## 4.3 激活函数的选择
激活函数的选择对于神经网络的训练和性能有重要影响。以下是一些常见的选择准则：
1. **隐藏层**：通常使用ReLU或其变种（如Leaky ReLU、Swish），因为它们计算简单且能有效缓解梯度消失问题。
2. **输出层**：
    - **回归问题**：使用线性激活函数。
    - **二分类问题**：使用Sigmoid函数。
    - **多分类问题**：使用Softmax函数


# 5 训练过程
假设我们有一个简单的神经网络，包含一个输入层、一个隐藏层和一个输出层：
1. 输入层**：2个节点，表示输入特征 $x_1$ 和 $x_2​$。
2. **隐藏层**：2个节点，表示隐藏状态 $h_1$ 和 $h_2$​，偏置向量分别为$b_1$、$b_2$,使用ReLU激活函数。
3. **输出层**：1个节点，表示输出 $y$，偏置向量分别为$b_3$,使用线性激活函数。

**权重矩阵**
1. **输入层到隐藏层的权重**：假设权重为 $W^{(1)} = \begin{bmatrix} W_{11} & W_{12} \\ W_{21} & W_{22} \end{bmatrix}$,其中 $W_{11}​$、$W_{12}$​ 连接 $x_1$​ 到 $h_1$​ 和 $h_2$​，$W_{21}​$、$W_{22}$​ 连接 $x_2$​ 到 $h_1$​ 和 $h_2​$。

2. **隐藏层到输出层的权重**：假设权重为 $W^{(2)} = \begin{bmatrix} W_{31} & W_{32} \end{bmatrix}$
    - $W_{31}$​ 和 $W_{32}$分别连接$h_1$ 和$h_2​$到 $y$。其中 $W_{31}$、$W_{32}$分别连接 $h_1$和 $h_2$到 $y$

如果指定具体数据，可以设置为
- 输入数据为$\mathbf{X} = [0.5, 0.6]$
- 隐藏层权重矩阵 $\mathbf{W}^{(1)} = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{bmatrix}$

- 隐藏层偏置向量 $\mathbf{b}^{(1)} = \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix}$

- 输出层权重矩阵 $\mathbf{W}^{(2)} = \begin{bmatrix} 0.5 & 0.6 \end{bmatrix}$
- 输出层偏置向量$\mathbf{b}^{(2)} = \begin{bmatrix} 0.3 \end{bmatrix}$


在训练过程中，通过调整权重和偏置，使得模型的预测结果尽可能准确。以下的主要步骤.

## 5.1 前向传播（Forward Propagation）
前向传播是数据从输入层经过隐藏层传递到输出层的过程。在这个过程中，每一层的节点接收前一层的输出，进行加权求和，并通过激活函数生成输出。
具体步骤如下：
- **加权求和**：每个节点接收前一层所有节点的输出，计算加权和。 $z_i = \sum_{j} w_{ij} x_j + b_i$​ 其中，$z_i$ 是第 $i$ 个节点的加权和，$w_{ij}$ 是从第 $j$ 个输入到第 $i$ 个节点的权重，$x_j$是第 $j$ 个输入，$b_i$​ 是偏置。
- **激活函数**：对加权和应用激活函数，生成节点的输出。 $a_i = f(z_i)$常用的激活函数包括Sigmoid、Tanh和ReLU。

### 5.1.1 输入层

输入层接收外部数据，将其传递给第一个隐藏层。假设输入数据为 $\mathbf{X} = [x_1, x_2]$
### 5.1.2 隐藏层
1. 计算隐藏层的输入加权和：
   $z_1^{(1)} = w_{11}x_1 + w_{12}x_2 + b_1$

   $z_2^{(1)} = w_{21}x_1 + w_{22}x_2 + b_2$

   代入具体数据：$z^{(1)} = \mathbf{W}^{(1)} \mathbf{X} + \mathbf{b}^{(1)} = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{bmatrix} \begin{bmatrix} 0.5 \\ 0.6 \end{bmatrix} + \begin{bmatrix} 0.1 \\ 0.2 \end{bmatrix} = \begin{bmatrix} 0.1 \cdot 0.5 + 0.2 \cdot 0.6 + 0.1 \\ 0.3 \cdot 0.5 + 0.4 \cdot 0.6 + 0.2 \end{bmatrix} = \begin{bmatrix} 0.27 \\ 0.62 \end{bmatrix}$
2. 应用激活函数，计算隐藏层的输出：
   $h_1 = \text{ReLU}(z_1^{(1)})$

   $h_2 = \text{ReLU}(z_2^{(1)})$


    $h^{(1)} = \text{ReLU}(z^{(1)}) = \begin{bmatrix} \max(0, 0.27) \\ \max(0, 0.62) \end{bmatrix} = \begin{bmatrix} 0.27 \\ 0.62 \end{bmatrix}$

### 5.1.3 输出层

1. 计算输出层的输入加权和：
   $z^{(2)} = w_{31}h_1 + w_{32}h_2 + b_3$
   $z^{(2)} = \mathbf{W}^{(2)} a^{(1)} + \mathbf{b}^{(2)} = \begin{bmatrix} 0.5 & 0.6 \end{bmatrix} \begin{bmatrix} 0.27 \\ 0.62 \end{bmatrix} + \begin{bmatrix} 0.3 \end{bmatrix} = 0.5 \cdot 0.27 + 0.6 \cdot 0.62 + 0.3 = 0.735$
2. 应用激活函数，计算最终输出：
   $y = z^{(2)}$
   假设输出层使用线性激活函数（即不做非线性变换）
   $y = z^{(2)} = 0.735$
## 5.2 计算损失（Loss Calculation）

使用损失函数计算预测输出与真实标签之间的差异。损失函数是一个衡量模型预测误差的指标，常见的损失函数包括均方误差（MSE）和交叉熵损失。

- **均方误差（MSE, Mean Squared Error）**：用于回归问题。 $\text{MSE} = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2$

- **交叉熵损失（Cross-Entropy Loss）**：用于分类问题。 $\text{Cross-Entropy} = -\sum_{i=1}^N y_i \log(\hat{y}_i)$

## 5.3 反向传播（Backpropagation）

反向传播算法通过计算损失函数对每个模型参数（权重和偏置）的偏导数/梯度，来指导参数更新，使得损失函数逐步减小，从而提高模型的准确性。

- **计算梯度**：首先计算输出层节点的损失梯度，即损失函数对输出层每个节点输出的偏导数。然后通过链式法则，依次计算每个隐藏层节点的梯度。梯度由后一层节点的梯度和当前层节点的输出值共同决定。
- **传播误差**：误差从输出层逐层传播回输入层，计算每个参数的梯度。

∂ 是偏导数符号


###  5.3.1 偏导数和梯度
偏导数表示在固定其他变量的情况下，一个变量的变化率。假设 $f(x, y)$ 是一个关于 $x$ 和 $y$的函数，则 $f$ 对 $x$ 的偏导数记作 $\frac{\partial f}{\partial x}$。

在神经网络中，偏导数用于计算梯度，帮助反向传播算法更新权重。梯度是损失函数关于每个参数的导数，表示损失函数变化率。对于一个权重 $w$，梯度 $\frac{\partial L}{\partial w}$ 表示权重变化对损失函数 $L$ 的影响。
具体而言，梯度表示损失函数相对于每个参数的偏导数：

$\frac{\partial L}{\partial W} = \text{梯度}$

通过计算每个参数的偏导数，反向传播算法能逐步调整网络权重，使得损失函数 $L$ 最小化，提高模型的预测能力。

### 5.3.2 偏导数推导过程

如果 $f(x, y) = x^2 + y^2$，则对 $x$ 的偏导数为：

$\frac{\partial f}{\partial x} = \frac{\partial (x^2 + y^2)}{\partial x} = 2x$

根据求导法则，分开对每一项求导：
$\frac{\partial f}{\partial x} = \frac{\partial (x^2 + y^2)}{\partial x} =\frac{\partial}{\partial x} (x^2) + \frac{\partial}{\partial x} (y^2)$

对于 $x^2$，使用幂函数求导法则 $\frac{\partial}{\partial x} (x^n) = nx^{n-1}$：
$\frac{\partial}{\partial x} (x^2) = 2x^{2-1}= 2x$
对于 $y^2$，因为$y$ 被视为常数，对 $x$ 求导结果为 0：
$\frac{\partial}{\partial x} (y^2) = 0$

同理，对 $y$ 的偏导数为：

$\frac{\partial f}{\partial y} = \frac{\partial (x^2 + y^2)}{\partial y} = 2y$

现在根据以上理解逐层反向计算每个参数的梯度

### 5.3.3  计算输出层的梯度

假设我们使用均方误差（MSE）作为损失函数：
$L = \frac{1}{2} (y - t)^2$

其中，$t$ 是目标值。

首先，计算损失相对于输出 $y$ 的梯度：

$\frac{\partial L}{\partial y} = y - t$

然后，计算损失相对于隐藏层到输出层权重 $W_{31}$​ 和 $W_{32}$的梯度：


$\frac{\partial L}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{31}}=\frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1 + w_{32}h_2 + b_3)}{\partial W_{31}} = (y - t) \cdot h_1$

由于 $W_{31}$和 $h_1$​ 相乘，而 $h_1$​  不依赖于 $W_{31}$，其余项在偏导数计算中都是常数，因此可以忽略。所以

在这个表达式中，$W_{31}$和 $h_1$​ 相乘，其余项与 $W_{31}$无关，因此在对 $W_{31}​$ 求导时可以忽略。
$\frac{\partial L}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{31}}=\frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1)}{\partial W_{31}}$

根据幂函数求导法则 $\frac{\partial}{\partial x} (x^n) = nx^{n-1}$
根据线性求导法则，常数项可以直接提取出来  $\frac{\partial}{\partial x} (a\cdot x) = a$

$\frac{\partial L}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{31}}=\frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1)}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1)}{\partial W_{31}}= (y-t) \cdot h_1$

同理可得：
$\frac{\partial L}{\partial W_{32}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{32}} = (y - t) \cdot h_2$

计算损失相对于偏置 $b_3​$ 的梯度：

$\frac{\partial L}{\partial b_3} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial b_3} = (y - t) \cdot 1 = y - t$

### 5.3.4 计算隐藏层的梯度

对于隐藏层的梯度，需要计算损失相对于隐藏状态 $h_1$​ 和 $h_2$​ 的梯度：

$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h_1} = (y - t) \cdot W_{31}$

$\frac{\partial L}{\partial h_2} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h_2} = (y - t) \cdot W_{32}$

其中，$y = w_{31}h_1 + w_{32}h_2 + b_3$

计算推导过程同上，可以得到以上结果

由于隐藏层使用的是ReLU激活函数，其导数为：

$\frac{\partial \text{ReLU}(z)}{\partial z} = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$

其中 $h_1 = \text{ReLU}(z_1^{(1)})$

因此应用链式法则后：

$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial h_1} \cdot \frac{\partial h_1}{\partial z_1} = (y - t) \cdot W_{31} \cdot \begin{cases} 1 & \text{if } z_1 > 0 \\ 0 & \text{if } z_1 \leq 0 \end{cases}$

$\frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial h_2} \cdot \frac{\partial h_2}{\partial z_2} = (y - t) \cdot W_{32} \cdot \begin{cases} 1 & \text{if } z_2 > 0 \\ 0 & \text{if } z_2 \leq 0 \end{cases}$

### 5.3.5 计算输入层的梯度

最后，计算损失相对于输入层权重 $W_{11}, W_{12}, W_{21}, W_{22}$​ 的梯度：

$\frac{\partial L}{\partial W_{11}} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_{11}} = \frac{\partial L}{\partial z_1} \cdot x_1$

$\frac{\partial L}{\partial W_{12}} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_{12}} = \frac{\partial L}{\partial z_2} \cdot x_1$

$\frac{\partial L}{\partial W_{21}} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_{21}} = \frac{\partial L}{\partial z_1} \cdot x_2$

$\frac{\partial L}{\partial W_{22}} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_{22}} = \frac{\partial L}{\partial z_2} \cdot x_2$​


## 5.4 更新权重（Weight Update）
使用梯度下降算法根据计算得到的梯度调整权重。梯度下降的基本公式为：
$W_{new​}=W_{old}​−η⋅\frac{\partial L}{\partial W}$

其中，$\eta$ 是学习率（Learning Rate），$\frac{\partial L}{\partial W}$​ 是损失函数对权重的梯度。

常见的梯度下降变种

- **批量梯度下降（Batch Gradient Descent）**：在整个训练数据集上计算梯度，然后更新权重。适用于小数据集，但计算量大，效率较低。
- **随机梯度下降（Stochastic Gradient Descent, SGD）**：在每个训练样本上计算梯度，然后更新权重。计算效率高，但梯度噪声大，收敛不稳定。
- **小批量梯度下降（Mini-Batch Gradient Descent）**：在小批量训练样本上计算梯度，然后更新权重。结合了批量和随机梯度下降的优点，常用在实际训练中。


# 7. 超参数（Hyperparameters） vs  模型参数（Parameters）

## **超参数（Hyperparameters）**：

- 定义：超参数是指在模型训练之前需要手动设置的参数，不通过训练数据学习得到，而是通过试验、经验或自动调优方法设定。
- 作用：控制模型的训练过程、模型复杂度、正则化程度等。
- 示例：学习率（learning rate）、批大小（batch size）、隐藏层的数量和大小、正则化系数（如L2正则化中的λ）、训练轮数（epochs）等。
-  调整方法：手动调试（Manual Tuning）、网格搜索（Grid Search）、随机搜索（Random Search）、贝叶斯优化（Bayesian Optimization）等。
- 调整频率：通常在训练之前设定，在训练过程中不变。可能需要多次试验和调优过程才能确定最佳超参数。

## **模型参数（Model Parameters）**：

- 定义：模型参数是指在模型训练过程中通过数据学习得到的参数，这些参数定义了模型的最终形态和行为。
- 作用：直接影响模型的预测输出，反映了模型从数据中学到的知识。
- 示例：神经网络中的权重和偏置、线性回归中的回归系数、支持向量机中的支持向量等。
- 调整方法：通过训练数据和优化算法（如梯度下降）自动调整。
- 调整频率：在每个训练迭代中都要更新，直到模型收敛或达到预设的训练轮数。

## 神经网络中的超参数和模型参数

**超参数**：

- 学习率（Learning Rate）：决定每次权重更新的步长。
- 批大小（Batch Size）：决定每次权重更新时使用的训练样本数量。
- 隐藏层数和每层神经元数量：定义神经网络的结构和复杂度。
- 正则化系数：控制正则化项在损失函数中的权重，防止过拟合。
- 训练轮数（Epochs）：模型在整个训练数据集上完整训练的次数。

**模型参数**：

- 权重（Weights）：连接神经元的权重，表示输入特征的重要性。
- 偏置（Biases）：每个神经元的偏置，用于调整激活函数的输出。


# 8. 序列数据 vs 非序列数据

## 1. 序列数据（Sequential Data）
序列数据（Sequential Data）是指按照时间或其他顺序排列的数据，其中每个数据点的意义和价值都依赖于它在序列中的位置和前后数据点的关系。序列数据广泛存在于许多实际应用中，如时间序列、自然语言处理、语音识别等。
### 序列数据的特点

1. **时间依赖性**：序列数据中的每个数据点与其前后数据点存在依赖关系。这种依赖性可以是短期的（仅依赖于最近的数据点）或长期的（依赖于较早的数据点）。
2. **顺序关系**：序列数据的顺序是至关重要的，数据点的顺序关系决定了其实际意义。例如，在语音信号中，音频帧的顺序决定了最终语音的内容。
3. **动态性**：序列数据往往是动态变化的，数据点的值随时间或其他顺序变化而变化。

递归神经网络（RNN）和其变体如LSTM和GRU擅长处理序列数据

## 2. 非序列数据
非序列化数据是指那些数据点之间没有时间或顺序依赖关系的数据。与序列化数据（如时间序列、文本、语音信号等）不同，非序列化数据中的每个数据点都是独立的，不依赖于前后的数据点。非序列化数据在各种领域中广泛存在，包括图像数据、表格数据（结构化数据）、图数据等。
不同类型的非序列化数据可以通过不同的神经网络进行处理，如卷积神经网络（CNN）处理图像数据，前馈神经网络（FNN）处理表格数据，图神经网络（GNN）处理图数据。在实际应用中，选择合适的神经网络模型能够有效地处理各种非序列化数据，解决实际问题。

### 非序列化数据的实际应用

#### 1. 图像数据的应用
图像数据是高维非序列数据，具有空间结构特性。卷积神经网络（CNN）是处理图像数据的主要神经网络类型。

**医疗影像分析**：通过CNN处理医疗影像（如MRI、CT图像），进行疾病诊断和分类。

**自动驾驶**：使用CNN分析汽车摄像头捕捉的道路图像，识别行人、交通标志和其他车辆。
**图像分类**：使用CNN对输入图像进行分类。例如，ImageNet数据集上的物体识别任务。
- 具体应用：卷积层提取图像的局部特征，池化层减少特征维度，全连接层进行分类。
- 典型模型：AlexNet、VGG、ResNet等。
**图像分割**：将图像划分为具有不同语义意义的区域。例如，自动驾驶中的道路标记识别。
- 具体应用：利用全卷积神经网络（FCN）或U-Net对图像进行像素级分类。
- 典型模型：U-Net、SegNet等。

#### 2. 表格数据的应用
表格数据通常存储在数据库或电子表格中，包含多种特征和目标变量。前馈神经网络（FNN）适用于处理表格数据。
**回归分析**：预测连续值，如房价预测。
- 具体应用：输入层接收多种特征，隐藏层提取特征之间的复杂关系，输出层给出预测值。
- 典型模型：多层感知器（MLP）。
-**分类任务**：对数据进行分类，如信用卡欺诈检测。
- 具体应用：输入层接收各特征值，隐藏层提取特征间关系，输出层进行分类。
- 典型模型：多层感知器（MLP）。


**客户分类**：使用FNN对客户进行分类，如根据客户购买行为预测客户流失风险。

#### 3. 图数据的应用
图数据由节点和边构成，具有复杂的连接结构。图神经网络（Graph Neural Networks, GNNs）专门用于处理图数据。

**节点分类**：在图中为每个节点分配标签，如社交网络中的用户分类。
- 具体应用：图卷积神经网络（GCN）通过聚合邻居节点的信息更新每个节点的表示，然后进行分类。
- 典型模型：GCN、GraphSAGE。
**图分类**：对整个图进行分类，如分子结构的化学性质预测。
- 具体应用：将图嵌入到固定长度的向量表示中，然后使用前馈神经网络进行分类。
- 典型模型：DGCNN、GraphSAGE。

**社交网络分析**：通过GNN分析社交网络中的用户关系，进行用户分类和推荐系统。

**化学分子建模**：使用GNN分析化学分子结构，预测分子的物理和化学性质。


# 9. 向量
在神经网络中，向量是一个重要的数学工具，用于表示和操作多个数值。向量在神经网络的各个部分都有广泛的应用，包括输入数据、权重、偏置、激活值等。为了更好地理解向量在神经网络中的角色，我们可以从以下几个方面进行详细阐述：

### 向量的定义

一个向量是一个具有方向和大小的数量集合，通常用一维数组来表示。在神经网络中，向量可以用来表示输入特征、隐藏层的激活值、输出值以及模型的权重和偏置。

### 向量在神经网络中的具体应用

#### 1. 输入向量

输入向量表示神经网络接收到的原始数据。在一个简单的前馈神经网络中，输入向量通常是一个包含多个特征的数据点。例如，对于一个图像分类任务，每个输入向量可能代表一张图像的像素值。

**示例**： 对于一个具有三个特征的输入数据点$(x_1, x_2, x_3)$，输入向量可以表示为：$\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}$

#### 2. 权重向量

权重向量表示神经元之间的连接强度。在神经网络中，每个神经元的输出都是前一层神经元输出的加权和。权重向量决定了输入特征对输出的影响程度。

**示例**： 对于一个具有三个输入特征的神经元，其权重向量可以表示为： $\mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix}$

#### 3. 偏置向量

偏置向量是一个额外的参数，用于调整神经元的输出，使其能够更好地拟合数据。偏置向量与权重向量一起，影响每个神经元的输出。

**示例**： 对于一个具有三个输入特征的神经元，其偏置向量可以表示为： $\mathbf{b} = b$

#### 4. 激活值向量

激活值向量表示神经网络中每一层的输出。在前向传播过程中，输入向量与权重向量相乘并加上偏置向量，经过激活函数后得到的值即为激活值。

**示例**： 对于一个具有三个神经元的隐藏层，其激活值向量可以表示为： $\mathbf{a} = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix}$

### 向量操作

在神经网络中，常见的向量操作包括向量加法、向量乘法（点积）、标量乘法和向量的激活函数应用。

#### 1. 向量加法

向量加法是将两个向量的对应元素相加。假设有两个向量 $\mathbf{a}$ 和 $\mathbf{b}$，它们的向量加法表示为： $\mathbf{c} = \mathbf{a} + \mathbf{b}$
$\mathbf{c} = \begin{bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ a_3 + b_3 \end{bmatrix}$

#### 2. 向量乘法（点积）

向量点积是将两个向量的对应元素相乘并求和。假设有两个向量 a\mathbf{a}a 和 $\mathbf{b}$，它们的点积表示为：
$c = \mathbf{a} \cdot \mathbf{b}$
$c = a_1 \cdot b_1 + a_2 \cdot b_2 + a_3 \cdot b_3$

#### 3. 标量乘法

标量乘法是将向量的每个元素乘以一个标量。假设有一个向量 $\mathbf{a}$ 和一个标量 $k$，它们的标量乘法表示为： $\mathbf{b} = k \cdot \mathbf{a}$

$\mathbf{b} = \begin{bmatrix} k \cdot a_1 \\ k \cdot a_2 \\ k \cdot a_3 \end{bmatrix}$


### 示例：前向传播中的向量运算

以一个简单的两层神经网络为例，说明向量在前向传播中的应用。

**输入层**：输入向量 $\mathbf{x}$
$\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$


**隐藏层**：权重向量 $\mathbf{W}$ 和偏置向量 $\mathbf{b}$
$\mathbf{W} = \begin{bmatrix} w_{11} & w_{12} \\ w_{21} & w_{22} \end{bmatrix}$

$\mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}$

**计算隐藏层激活值**：
$\mathbf{z} = \mathbf{W} \cdot \mathbf{x} + \mathbf{b}$

$\mathbf{z} = \begin{bmatrix} w_{11}x_1 + w_{12}x_2 + b_1 \\ w_{21}x_1 + w_{22}x_2 + b_2 \end{bmatrix}$

**应用激活函数（如ReLU）**：
$\mathbf{a} = \text{ReLU}(\mathbf{z})$
$\mathbf{a} = \begin{bmatrix} \text{ReLU}(z_1) \\ \text{ReLU}(z_2) \end{bmatrix}$


**输出层**：权重向量 $\mathbf{W'}$ 和偏置 $\mathbf{b'}$
$\mathbf{W'} = \begin{bmatrix} w_{31} & w_{32} \end{bmatrix}$
$\mathbf{b'} = b'$

**计算输出值**：
$y = \mathbf{W'} \cdot \mathbf{a} + \mathbf{b'}$
$y = w_{31}a_1 + w_{32}a_2 + b'$

# 9. 误差
训练误差、测试误差和验证误差是三个不同的概念，它们分别衡量模型在不同数据集上的表现。这些误差帮助我们评估模型的拟合程度和泛化能力
## 9.1 区别和联系
- **训练误差**：衡量模型在训练数据上的表现，主要用于训练过程中调整模型参数。
- **验证误差**：衡量模型在验证数据上的表现，主要用于超参数调优和模型选择。验证数据是从训练数据中分离出来的一部分，不参与模型训练。
- **测试误差**：衡量模型在测试数据上的表现，主要用于评估模型的最终泛化能力。测试数据在训练和验证过程中都不使用，只有在模型训练完成后才用于评估。
### 9.2 训练误差（Training Error）

训练误差是指模型在训练数据上的误差。它反映了模型对训练数据的拟合程度。
**计算方法**

训练误差通常通过在训练数据上计算损失函数（例如均方误差、交叉熵损失等）来得到。例如，如果使用均方误差（MSE）作为损失函数，训练误差可以表示为：

$\text{MSE}_{\text{train}} = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} (y_i - \hat{y}_i)^2$


其中，NtrainN_{\text{train}}Ntrain​ 是训练数据的样本数量，yiy_iyi​ 是第 iii 个样本的真实值，y^i\hat{y}_i y^​i​ 是模型对第 iii 个样本的预测值。

**目标**
最小化训练误差，以便模型能够良好地拟合训练数据。

**意义**
低训练误差表明模型能够很好地拟合训练数据。但这并不一定意味着模型在新数据上的表现也会良好。
## 9.2 预测误差/测试误差

预测误差/测试误差 是指模型在未见过的数据（通常是测试数据或验证数据）上的误差。它反映了模型的泛化能力，即模型在新数据上的表现。

- **计算方法**：在模型训练完成后，使用测试数据或验证数据计算损失函数的值，计算方法与训练误差类似
- **目标**：评估模型的泛化能力，期望模型在测试数据上的误差尽可能低。
- **意义**：低预测误差表明模型具有良好的泛化能力，能够在新数据上表现良好。

## 9.3 误差的作用

训练误差和预测误差的关系可以帮助我们诊断模型的状态，判断模型是否过拟合或欠拟合。

- **欠拟合（Underfitting）**：模型在训练数据和测试数据上的误差都很高，说明模型复杂度不足，无法捕捉数据中的规律。
- **合适拟合（Good Fit）**：模型在训练数据上的误差较低，并且在测试数据上的误差也较低，说明模型具有良好的泛化能力。
- **过拟合（Overfitting）**：模型在训练数据上的误差很低，但在测试数据上的误差很高，说明模型过于复杂，捕捉到了训练数据中的噪声和细节，泛化能力较差。


# 10.过度拟合
过度拟合（Overfitting）是机器学习中的一个常见问题，指的是模型在训练数据上表现良好，但在未见过的测试数据或实际应用中表现不佳。这通常是因为模型过于复杂，捕捉到了训练数据中的噪声和偶然性模式，而不是数据的潜在规律。

### 过度拟合的具体表现

1. **训练误差低，测试误差高**：模型在训练数据上的误差很低，但在测试数据或新数据上的误差很高。
2. **高方差**：模型对训练数据中的细节和噪声过于敏感，导致对不同数据集的表现差异很大。
3. **复杂模型**：过于复杂的模型（例如，具有太多参数的深度神经网络）容易过度拟合。

### 过度拟合的原因
1. **模型复杂度高**：模型的参数过多，能够拟合训练数据中的每一个细节和噪声。
2. **训练数据不足**：训练数据量过少，模型无法学习到数据的真实分布和规律。
3. **噪声数据**：训练数据中包含大量噪声，模型将这些噪声误认为是数据的潜在模式。
4. **缺乏正则化**：没有使用正则化技术来约束模型的复杂度。

### 如何检测过度拟合

1. **训练误差与验证误差**：在训练过程中，观察训练误差和验证误差的变化。如果训练误差持续下降，而验证误差在某个点之后开始上升，这通常是过度拟合的信号。
2. **交叉验证**：使用交叉验证技术评估模型在多个数据子集上的表现，避免模型对单一训练集的过度依赖。
3. **学习曲线**：绘制学习曲线（训练误差和验证误差随训练样本数量变化的曲线），分析模型的学习行为。

### 解决过度拟合的方法

1. **增加训练数据**：通过增加训练数据量，模型可以更好地学习数据的真实分布，减少对噪声的拟合。
2. **简化模型**：减少模型的参数数量或选择更简单的模型，避免过度拟合。
3. **正则化**：使用正则化技术（如L1和L2正则化）来约束模型参数，使其更平滑，减少对训练数据的过度拟合。
    - **L1正则化**：通过对模型参数的绝对值求和，使部分参数变为零，起到特征选择的作用。
    - **L2正则化**：通过对模型参数的平方和进行约束，使参数值尽可能小，从而使模型更平滑。
4. **Dropout**：在训练过程中随机丢弃一部分神经元，防止模型对某些路径的过度依赖。
5. **数据增强**：通过对训练数据进行旋转、缩放、裁剪等变换，生成更多的训练样本，增加数据的多样性。
6. **早停法（Early Stopping）**：在训练过程中监控验证误差，当验证误差不再下降时，提前停止训练，防止模型过度拟合。
# 11. 泛化能力
泛化能力（Generalization）是指机器学习模型在训练数据以外的数据（通常是未见过的测试数据或真实应用中的数据）上表现良好的能力。它反映了模型对数据的普遍规律的学习程度，而不是对训练数据的记忆程度。

一个具有良好泛化能力的模型能够有效地从训练数据中学习到潜在的规律，并将这些规律应用于新数据上，从而在实际应用中保持高效和准确的表现。

理解泛化能力需要考虑以下几个方面：

1. 训练误差和测试误差
   如果模型在训练数据上的误差很低，但在测试数据上的误差很高，这通常表明模型过度拟合（Overfitting）。相反，如果模型在训练数据和测试数据上的误差都较低，这表明模型具有良好的泛化能力。
2. 模型复杂度
    - **简单模型**：模型复杂度低，参数较少，容易欠拟合（Underfitting），即无法充分捕捉数据中的规律。
    - **复杂模型**：模型复杂度高，参数较多，容易过度拟合，即捕捉了训练数据中的噪声和偶然模式。
      一个具有良好泛化能力的模型应在简单和复杂之间取得平衡，既能捕捉数据的潜在规律，又不过度拟合噪声。

# 12.正则化
正则化（Regularization）是一种在机器学习和统计学中用于防止模型过拟合（overfitting）的技术,提高模型泛化能力的关键技术。
#### 正则化的类型
1. L1 正则化（Lasso 正则化）*
    - 定义：在损失函数中添加所有模型参数绝对值的和。
    - 数学表达： $\text{Loss} = \text{Original Loss} + \lambda \sum |w_i|$
    - 特点：可以导致一些参数完全为零，起到特征选择的作用。
2. L2 正则化（Ridge 正则化）：
    - **定义**：在损失函数中添加所有模型参数平方和。
    - **数学表达**： $\text{Loss} = \text{Original Loss} + \lambda \sum w_i^2$
    - **特点**：可以防止参数变得过大，但不会使参数完全为零。
3. **弹性网络（Elastic Net）正则化**：
    - **定义**：结合 L1 和 L2 正则化。
    - **数学表达**：$\text{Loss} = \text{Original Loss} + \lambda_1 \sum |w_i| + \lambda_2 \sum w_i^2$
    - **特点**：结合了 L1 和 L2 的优点，既可以选择特征又可以防止参数过大。
4. **Dropout 正则化**：
    - **定义**：在每次训练时随机丢弃一部分神经元，使得模型在训练过程中不会过于依赖某些特定的神经元。
    - **特点**：通过在训练过程中引入随机性，增强模型的鲁棒性。

#### 正则化的原理
- **复杂度惩罚**：通过向损失函数中添加一个表示模型复杂度的项，模型在训练时不仅要最小化原始损失函数，还要考虑模型的复杂度。
- **参数约束**：限制模型参数的大小或数量，防止模型在训练数据上过度拟合。
- **增强泛化能力**：通过控制模型的复杂度，提升模型在未见数据上的表现。

#### 正则化在模型中的应用

正则化技术在许多机器学习模型中应用广泛，包括线性回归、逻辑回归、支持向量机（SVM）、神经网络等。在实际应用中，正则化参数（如 λ\lambdaλ）通常需要通过交叉验证等方法进行调优，以获得最佳的模型性能。