
ChatGPT是由OpenAI开发的，一个能够理解和生成自然语言的人工智能（AI）模型，可以和用户进行互动并生成类似人类的对话。

![1.png](images%2FIntro-to-chatGPT-%E4%BB%8EG%E3%80%81P%E3%80%81T%E5%90%AB%E4%B9%89%E8%A7%A3%E9%87%8AchatGPT%2F1.png){% asset_img 1.png %}

# 1. chatGPT 的发展历程
ChatGPT模型的发展历程是一个不断演进和改进的过程。以下是关键的时间节点和发展阶段：

1. **GPT-1（2018年6月）**：OpenAI发布了首个生成预训练变换器模型（Generative Pre-trained Transformer，GPT-1）。该模型基于Transformer架构，使用无监督学习方法在大规模文本语料上进行预训练，然后在特定任务上进行微调（Fine-tuning）。
    - **架构**：12层Transformer解码器，参数量约为1.17亿。
    - **创新点**：引入无监督预训练和有监督微调相结合的训练方法，在多个NLP任务上表现优异。
1. **GPT-2（2019年2月）**：OpenAI发布了GPT-2，模型规模和能力大幅提升。最初由于担心模型被滥用，OpenAI仅发布了部分参数的模型，后于2019年11月发布了完整模型。
    - **架构**：最大版本有48层，参数量达15亿。
    - **创新点**：显著提高了模型的生成质量和连贯性，在文本生成、翻译、问答等任务上表现出色。
1. **GPT-3（2020年6月）**：是当时最大和最强大的语言模型，包含1750亿参数。
    - **架构**：1750亿参数，96层，采用更大规模的数据进行训练。
    - **创新点**：通过超大规模预训练和少量示例（Few-shot Learning），在无需微调的情况下，也能在多个任务上取得惊人的效果。
4. **GPT-3.5（2021年11月）**：
    - 是GPT-3的改进版本，模型参数达到2000亿，利用人类反馈进行强化学习，进一步提升模型的交互能力。
    - 增强了处理复杂对话和多轮对话的能力。
6. **GPT-4（2023年2月）**：
    - 尚未公开具体参数，但推测远超GPT-3.5，支持多模态输入和输出
    - 提供了更高水平的自然语言理解和生成能力，支持多模态输入与输出
- **GPT-4o(2024年5月)**
    - 工程能力大幅提升， 体现在相应速度更快，且开始能理解语音语调

ChatGPT模型的发展不仅仅是参数的增加，更是算法优化、数据多样化和对用户反馈的持续改进。

目前我们使用的chatGPT 后GPT-3.5、GPT-4、GPT-4o 三个版本可选。

# 2. G、P、T分别是什么意思
GPT 是 Generative Pre-trained Transformer 的缩写。以下内容将重点介绍G、P、T 三个字母各自的含义，以此来更好得理解chatGPT 是什么。
## 2.1 Generative  生成式

ChatGPT是一个Generative AI, 即生成式AI。

AI 作为一个总称，其实包含非常多具体的类型， ChatGPT 所属的Generative  AI 是其中一个子类。

日常生活中会用到的siri、小度、小爱、识别图片中动物是小猫、医院的专家诊断系统、和你国际象棋对战的机器人，这些都是AI 。 但是这些AI 都是规则驱动的系统，只能根据预设规则进行回答，一旦超出预设范围，就会表现的人工智障。

chatGPT作为一个Generative AI, 其最大的特点就是根据输入生成新内容。它不仅能回答问题，还能进行创意写作、故事生成、诗歌创作等多种任务。

Generative AI 也有自己具体的分类

1. Natural language generation
   chatGPT 就是 NLG, 可以说是目前最著名的AI 应用
2.  text to image
    根据文字生成图片
    1. Midjourney
    2. DALL-E
    3. Stable Diffusion
3.  Generative Adversarial Networks (GANs)
    生成对抗网络，其核心是两个相互对抗的网络：生成器（Generator）和判别器（Discriminator）。这两个网络在训练过程中相互竞争，从而不断提升自身的性能。
    1. **生成器（Generator）** - 这个网络的任务是捕捉训练数据的分布，并生成尽可能接近真实数据的新数据。生成器接受一个随机噪声向量作为输入，通过这个噪声向量构造出新的数据实例。
    2. **判别器（Discriminator）** - 判别器的任务是区分输入给它的数据是来自训练集（即真实数据）还是生成器生成的假数据。基本上，判别器是一个二分类模型，输出一个标量表示输入数据是真实数据的概率。
       这两个模型在训练过程中进行对抗。生成器试图产生越来越真实的数据以“欺骗”判别器，而判别器则试图变得更好地区分真假数据。通过这种对抗过程，生成器学会生成高质量的数据。
4. VAEs
   可用于异常检测，方法是在正常数据的数据集上训练模型，然后使用经过训练的模型来识别偏离正常数据的实例。这可用于检测各种情况下的异常情况，例如发现金融交易中的欺诈行为、发现制造中的缺陷或发现网络中的安全漏洞。例如，Uber 在其金融交易中使用 VAE 进行异常检测，以检测欺诈行为。


## 2.2 Pre-trained 预训练

预训练 是chatGPT训练 的第一个阶段。

chatGPT 的 训练可以分为以下几个阶段
1. pre-training
2. supervised fine-tuning
3. reward modeling
4. reinforcement learning

其中pre-training是第一个阶段, 在预训练阶段使用的数据集通常是从互联网上收集的大量文本，这些文本可能包含多种语言、主题和格式。

预训练的目标是让模型在大量的文本数据上进行了广泛的训练学习了语言的结构、语法以及大量的知识。学习结束后， 预训练 阶段会产出一个base model, base model 可以根据 用户的输入， 去续写文本， 注意， 这个阶段的chatGPT 还不能回答你的问题。

针对以下问题输入，，比如， 它只能给出这样的回答。
what is the capital of China?

base model 并不会回答问题，给出“beijing”, 而是会续写内容， 给出以下可能的答案

what is China's largest city?
what is China's population?
what is the currency of China?

如果想让模型“理解”人类的问题并进行回答， 还需要进行第二阶段的训练 supervised fine-tuning。

## 2.3 Transformer
ChatGPT的核心技术是Transformer架构，这是一种深度学习模型，擅长处理序列数据。Transformer通过自注意力机制（self-attention mechanism）来捕捉输入文本中的重要特征和上下文关系。这种架构使得模型在处理长文本和复杂上下文时，能够保持较高的准确性和连贯性。

要理解Transformer在ChatGPT中的应用，我们可以用一个简单的类比来说明。想象一下，你在读一本书，并试图理解每一段的意思。你的大脑不仅仅是逐字逐句地阅读，还会结合前后的内容，理解整段的意义。Transformer在ChatGPT中的作用就像你大脑的这种理解机制。


# 3. chatGPT 使用场景
发挥你的想象，能用的具体场景是在太多了。后面这里会陆续补充我使用的场景
