在学习AI 过程中，发现专业名词相当多，初学者可能会感到而混乱，所以本篇内容是对该领域内的一些“大词”进行简单介绍， 做一些概念扫盲， 以保证在接下来的学习中心中有框架。 正式内容会按照下图框架介绍

![1.png](images%2FIntro-to-AI%2F1.png){% asset_img 1.png %}

## 1. 人工智能 (Artificial Intelligence)
人工智能（Artificial Intelligence，简称AI）是一门研究和开发用于模拟、扩展和扩展人类智能的理论、方法、技术及应用系统的科学技术。

简单来说，AI指的是使计算机系统能够执行通常需要人类智能才能完成的任务。这些任务包括学习、推理、解决问题、感知、语言理解和生成等。

人工智能（AI）的发展历史可以追溯到20世纪中期，它的发展经历了多个重要阶段，每个阶段都有其独特的特点和里程碑事件。下面来简单介绍其发展历程

### 1.1 早期阶段：基础理论和初步探索（1950s-1960s）
**1956年达特茅斯会议**：AI正式诞生。1956 年，John McCarthy、Marvin Minsky、Allen Newell 和 Herbert A. Simon 等研究者在达特茅斯会议上首次提出了“人工智能”这一术语，标志着AI研究正式成为一个独立的学科。

**1950年代末**：艾伦·图灵提出了“图灵测试”，成为判断机器是否具备智能的基准。

这一时期，由于技术和硬件的限制，早期的AI系统主要关注于简单任务的自动化，如象棋和跳棋游戏。例如，IBM 的 Deep Blue 和 MIT AI Lab 的 MacHack VI 等系统在象棋游戏中取得了显著的成就。这些系统虽然在处理复杂模式方面受限，但它们的成功展示了AI在规则清晰、限定环境中的潜力。

### 1.2 专家系统与机器学习（1970s-1990s）

**1970年代**：专家系统（Expert Systems）开始兴起，如MYCIN用于医疗诊断，DENDRAL用于化学分析。这些系统能够储存、解释和推理知识,可以用来解决特定领域的问题。

**1980s**：反向传播算法（Backpropagation）的提出使神经网络重新受到关注。尽管计算能力仍有限，但理论和方法上的突破为未来的发展奠定了基础。

**1990s**: 随着更多的数据可用和计算能力的增加，机器学习方法，特别是基于统计的方法开始主导AI研究。支持向量机和随机森林等技术的发展，为AI在图像识别、自然语言处理和其他复杂模式识别任务中的应用打开了新的可能性。

### 1.3 现代AI：大数据与深度学习时代（2000s-至今）

1. **计算能力和数据的提升**：
    - **2000s**：互联网和大数据的发展，为AI提供了大量训练数据。计算能力的提升，特别是GPU的使用，使得复杂的模型训练成为可能。
2. **深度学习的崛起**：
    - **2010s**：深度学习（Deep Learning）在图像识别、语音识别、自然语言处理等领域取得突破。AlexNet在2012年ImageNet竞赛中的成功，标志着深度学习的重大胜利。
3. **广泛应用**：
    - **2010s-至今**：AI在自动驾驶、医疗诊断、金融分析、智能客服等领域得到广泛应用。2016年，AlphaGo击败围棋冠军李世石，展示了AI在复杂博弈中的强大能力。
    - **自然语言处理**：GPT-3等大型语言模型的推出，使得AI能够生成逼真的自然语言文本，应用于翻译、对话系统、内容创作等多个领域。

## 2. 机器学习 与 深度学习
机器学习（Machine Learning）和深度学习（Deep Learning）是现代人工智能（Artificial Intelligence）领域的核心技术。它们的发展极大地推动了从图像识别到自然语言处理的各种应用。
### 2.1 机器学习（Machine Learning）
机器学习是一种使计算机能够通过经验自动改进的技术。它依赖于算法和统计模型，使得计算机系统可以识别数据中的模式并做出决策，无需明确编程。

**主要类别**：
1. **监督学习（Supervised Learning）**：模型在带有标签的数据集上进行训练，学习输入与输出之间的映射关系。应用包括分类（Classification）和回归（Regression）。
2. **无监督学习（Unsupervised Learning）**：模型在没有标签的数据集上工作，目标是发现数据的内在结构。常见的任务有聚类（Clustering）和降维（Dimensionality Reduction）。
3. **半监督学习（Semi-supervised Learning）**：使用部分标记的数据进行训练，结合监督学习和无监督学习的特点。
4. **强化学习（Reinforcement Learning）**：模型通过与环境的交互学习策略，目标是最大化某种数值奖励（Reward）。

### 2.2 深度学习（Deep Learning）

深度学习是机器学习中的一个子集，它使用称为人工神经网络（Artificial Neural Networks）的模型，特别是具有多个层（Layers）的深层网络，以学习数据的高级抽象特征。

**核心概念**：
- **神经网络（Neural Networks）**：一个由节点（或称为神经元，Neurons）组成的网络，节点在层中组织并通过激活函数（Activation Functions）处理信息。
- **卷积神经网络（Convolutional Neural Networks, CNNs）**：特别适合处理图像数据。
- **循环神经网络（Recurrent Neural Networks, RNNs）**：优秀的处理序列数据如时间序列或自然语言的工具。
- **长短期记忆网络（Long Short-Term Memory, LSTM）**和**门控循环单元（Gated Recurrent Units, GRU）**：是RNN的变体，解决了传统RNN长期依赖问题。
- **Transformer**：一种基于自注意力机制（Self-attention Mechanism）的架构，广泛用于自然语言处理领域。

### 2.3  区别与联系

**联系**：
- 深度学习是机器学习的一种特殊形式，利用复杂的神经网络结构来解决广泛的问题。
- 两者都依赖数据来学习，并通过迭代过程改进模型性能。

**区别**：
- 机器学习包括一系列不仅限于神经网络的技术和方法。
- 深度学习通常需要更大量的数据和更强的计算能力。

## 3. 神经网络 (Neural Networks)

神经网络是实现AI 的一种技术手段，一种广泛用于机器学习（Machine Learning）和深度学习（Deep Learning）领域的计算模型/算法架构。

它受到人类大脑神经元（Neurons）和它们的互动方式的启发，它由多个层（Layers）组成，每层包含多个神经元，这些神经元通过权重（Weights）连接传递信息。

神经网络的训练过程基于机器学习的基本前提，即能够从数据中学习。通过向网络提供大量的数据样本（包括输入和期望的输出），神经网络可以学习到如何映射输入到输出，这种能力是通过调整内部结构（即权重）来实现的。

这一学习过程使用了机器学习中的核心概念，如损失函数（Loss Functions）、梯度下降（Gradient Descent）和反向传播算法（Backpropagation Algorithms）。这些都是机器学习领域的基本工具，用于训练模型以改进其性能。

## 4. Transformer
Transformer（变换器）是一种革命性的神经网络架构，它在自然语言处理（Natural Language Processing, NLP）和其他序列建模任务中取得了显著的成就。Transformer 最初由 Vaswani 等人在 2017 年的论文 "Attention Is All You Need" 中提出，其核心思想是完全依靠注意力机制（attention mechanisms），摒弃了之前常用的循环神经网络（Recurrent Neural Networks, RNNs）和卷积神经网络（Convolutional Neural Networks, CNNs）中的结构。

## 5. NLP
自然语言处理（Natural Language Processing，简称NLP）是人工智能的一个分支，致力于让计算机理解、解释和生成人类语言。它结合了计算机科学、人工智能和语言学的知识与技术，用于处理和分析大量的自然语言数据。

NLP使用各种技术来处理和理解语言。这些方法从规则基础的方法到基于机器学习的方法，特别是深度学习，都有涵盖。随着时间的推移，深度学习在NLP中变得越来越重要，因为它能够在处理自然语言的复杂性方面提供显著的改进。

**传统方法：**
- **基于规则的系统**：使用预定义的语言规则来解释文本。
- **统计模型**：基于大量语料库数据，使用统计方法推断和预测。

**现代方法**
- **神经网络**：使用多层神经网络模型处理语言任务，如循环神经网络（RNN）、长短期记忆网络（LSTM）和最近的变换器模型（如BERT、GPT）。

NLP技术被广泛应用于各种现实世界的场景和产品中，例如：
- **聊天机器人**和**虚拟助手**（如Apple的Siri、Google Assistant、Amazon Alexa）。
- **文本分析工具**，帮助企业监测和分析社交媒体上的消费者情绪。
- **电子邮件过滤**和**反垃圾邮件技术**。
- **语音到文本服务**，如在法庭记录或医疗记录系统中自动转录口述内容。

## 5. LLM
LLM（Large Language Models，大型语言模型）是自然语言处理（Natural Language Processing, NLP）领域的一种先进技术，主要依赖于深度学习（Deep Learning）技术，特别是基于 Transformer 架构的神经网络模型。这些模型因其规模庞大和在多种语言任务上的出色表现而得名。

LM 是设计用来理解、生成、翻译、摘要等处理文本的大规模神经网络模型。它们通常包含数十亿至数万亿个参数，并在大量多样化的文本数据上进行训练，以学习语言的深层次结构和语义。
**核心组件**
1. **Transformer 架构（Transformer Architecture）**：
    - LLM 多使用基于 Transformer 的模型，这种模型依靠自注意力机制（Self-Attention Mechanism）来处理文本数据，优于传统的循环神经网络（Recurrent Neural Networks, RNNs）或卷积神经网络（Convolutional Neural Networks, CNNs）。
2. **预训练与微调（Pre-training and Fine-tuning）**：
    - **预训练（Pre-training）**：在大规模未标记数据上进行，模型学习语言的通用特征。
    - **微调（Fine-tuning）**：在特定任务的较小标记数据集上进行，调整模型以适应具体应用。
3. **自监督学习（Self-supervised Learning）**：
    - LLM 通常通过自监督学习预训练，这意味着它们使用输入数据的不同部分作为自己的监督信号，例如，预测文本中被遮蔽（Masked）的单词。
